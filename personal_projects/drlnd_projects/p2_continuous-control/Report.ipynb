{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2. Continuous Control\n",
    "In this project, I trained a robot agent that reaches a ball using Deep Deterministic Policy Gradient (DDPG)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem Definition\n",
    "\n",
    "### Environment\n",
    "The problem environment is a 3d space. It has one or 20 robot agents and their target balls that are generated randomly around them. \n",
    "\n",
    "<img src=\"assets/reacher_random.gif\"/>\n",
    "\n",
    "### Task\n",
    "The agent's goal is to move its arm to reach the target ball and keep it there.\n",
    "\n",
    "### State\n",
    "The agent observes a state with 33 variables that contain position, rotation, velocity and angular velocities of the arm. \n",
    "\n",
    "### Action\n",
    "The agent's action is a vector of 4 numbers that are torque applicable to two joints.\n",
    "These numbers are ranged between -1 and 1. \n",
    "\n",
    "### Reward\n",
    "The agent gains +0.1 when its hand is touching the target ball.\n",
    "\n",
    "### Completion Target\n",
    "The task is considered solved when the average score gets better than 30 over 100 episodes.\n",
    "\n",
    "### Algorithm\n",
    "In this project I'm going to use DDPG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name='Reacher_multi.app')\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DDPG\n",
    "\n",
    "### Motivation\n",
    "Despite of its success with Atari Games, the use of Deep Q Network(DQN) is limited to tasks with discrete and low-dimensional action spaces. In the previous project, DQN agent solved LunarLander-v2 where it only had to choose \"Left\", \"Right\", \"Down\" and \"Do Nothing\". However, DQN is not suitable for tasks that have continuous and high dimensional action spaces due to the way it works. DQN learns to estimate values of each possible action, and updates its weights via choosing the action that maximizes the action-value function. To deal with infinite number of real-valued actions ranging from \"Left 50.0001m\" to \"Left 59.999m\" DQN's output layer would have infinite number of neurons, which is simply not feasible. One could discretize the continuous actions into a number of discrete actions. But doing so cannot avoid the curse of dimensionality and information loss.\n",
    "\n",
    "Deep Deterministic Policy Gradient is born to overcome such limitation. It is model-free, off-policy, actor-critic reinforcement learning algorithm that uses deep learning as a function approxiamtor. DDPG can handle continuous action spaces and learn policies from high dimensional input data.\n",
    "\n",
    "### Key Ideas\n",
    "DDPG combines actor-critic architecture with insights from DQN.   \n",
    " - DQN: DDPG's critic draws experience tuples from Experience Replay Buffer like DQN. Plus, each actor and critic network have their own target copies to update parameters.\n",
    "- Actor-Critic: DDPG architecture has two separate networks: actor network that outputs next action, and critic network that estimates its value.  \n",
    " - Actor: DDPG's actor uses deterministic policy that outputs best action for a given state $\\mu(s; \\theta^\\mu$). Thanks to the deterministic policy it can produce any real-valued actions via tanh or sigmoid activation at the end of the network.\n",
    " - Critic: DDPG's critic learns to evaluate the optimal action value function of the best action provided by the actor. $Q(s, \\mu(s;\\theta^\\mu);\\theta^Q)$ \n",
    " - Ornstein-Uhlenbeck process: The authors used Ornstein-Uhlenbeck process to add noise to encourage exploration of the agent. This process produces zero-mean values that are temporarily correlated to each other. \n",
    " - Soft Update: DQN's network gets a big update every 10,000 steps. DDPG suggests a much subtle way of updating parameters. At every step DDPG blends a tiny portion $\\tau$ of local network with the target network. This enables more stable learning.\n",
    "\n",
    " \n",
    "### Pseudo code\n",
    "- Randomly Initialze critic network $Q(s, a|\\theta^Q)$ and actor $\\mu(s|\\theta^\\mu)$ with weights $\\theta^Q$ and $\\theta^\\mu$\n",
    "- Initialize target network $Q^\\prime$ and $\\mu^\\prime$ with weights $\\theta^{Q^\\prime} \\leftarrow \\theta^Q$, $\\theta^{\\mu^\\prime} \\leftarrow \\theta^\\mu$\n",
    "- Initialize replay buffer\n",
    "- for episode = 1, M do\n",
    "  - Initialize a random process $N$ for action exploration\n",
    "  - Receive initial observation state $s_1$\n",
    "  - for t=1, T do\n",
    "    - Select action $a_t = \\mu(s_t|\\theta^\\mu) + N_t$ according to the current policy and exploration noise\n",
    "    - Execute action $a_t$ and observe reward $r_t$ and observe new state $s_{t+1}$\n",
    "    - Store transition $(s_t, a_t, r_t, s_{t+1})$ in $R$\n",
    "    - Sample a random minibatch of N transitions $(s_i, a_i, r_i, s_{i+1})$ from $R$\n",
    "    - Set $y_i = r_i + \\gamma Q^\\prime(s_{i+1}, \\mu^\\prime(s_{i+1}|\\theta^{\\mu^\\prime}) | \\theta^{Q^\\prime})$\n",
    "    - Update critic by minimising the loss: $L = \\frac{1}{N} \\Sigma_i(y_i - Q(s_i, a_i | \\theta^Q))^2$\n",
    "    - Update the actor policy using the sampled policy gradient:\n",
    "    $$\\triangledown_{\\theta^\\mu}J \\approx \\frac{1}{N} \\Sigma_i \\triangledown_a Q(s, a|\\theta^Q)|_{s=s_i, a=\\mu(s_i}\\triangledown_{\\theta^\\mu}\\mu(s|\\theta^\\mu)|_{s_i}$$\n",
    "    - Update the target networks:\n",
    "    $$\\theta^{Q^\\prime} \\leftarrow \\tau \\theta^Q + (1 - \\tau)\\theta^{Q^\\prime}$$\n",
    "    $$\\theta^{\\mu^\\prime} \\leftarrow \\tau \\theta^\\mu + (1 - \\tau)\\theta^{\\mu^\\prime}$$\n",
    "  - end for\n",
    "- end for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation Details\n",
    "\n",
    "### 3-1. Actor Network\n",
    "\n",
    "- takes state (33 variables) as input\n",
    "- outputs action vector that has 4 numbers that are ranged between -1 and +1 through tanh activation\n",
    "- has two hidden layers that have 256 and 128 neurons\n",
    "<img src=\"assets/actor_nn.png\" width=\"500px\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def hidden_init(layer):\n",
    "    \"\"\"\n",
    "    The weight initialization method used by the authors of DDPG paper\n",
    "    > The other layers were initialized from uniform distributions [-1/np.sqrt(f), 1/np.sqrt(f)]\n",
    "    > where f is the fan-in of the layer.\n",
    "    \"\"\"\n",
    "    fan_in = layer.weight.data.size(0)\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor Model\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, fc1_size=256, fc2_size=128):\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, fc1_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(fc2_size, action_size)\n",
    "        self.tanh_out = nn.Tanh()\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.tanh_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Critic Network\n",
    "- takes state (33 variables) as input\n",
    "- concat the output of the first hidden layer and actions from Actor Network before the second hidden layer\n",
    "- outputs a single value that represents the estimated value of the given state\n",
    "- two hidden layers have 256 and 128 neurons respectively\n",
    "<img src=\"assets/critic_nn.png\" width=\"500px\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic Network\"\"\"\n",
    "    def __init__(self, state_size, action_size, seed, fc1_size=256, fc2_size=128):\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, fc1_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc1_size + action_size, fc2_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(fc2_size, 1)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = self.fc1(state)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = torch.cat((x, action), dim=1)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. Actor-Critic\n",
    "1. observe $s_t$\n",
    "2. pick action $a_t$ from actor_local  \n",
    "  2-1. pick action  \n",
    "  2-2. add Orstein-Uhlenbeck noise  \n",
    "  2-3. clip from -1 to +1  \n",
    "3. get $r_{t+1}, s_{t+1}$  \n",
    "  3-1. append the experience tuple $(s_t, a_t, r_{t+1}, s_{t+1})$ to the replay buffer  \n",
    "  3-2. draw sample experience tuples from the replay buffer  \n",
    "  3-3. learn from the samples and run updates  \n",
    "    1. get action_next from actor_target with $s_{t+1}$ (predict the next action)  \n",
    "    2. calculate value of action_next with $s_{t+1}$ via critic_target (estimate the value of that action)  \n",
    "    3. get the target estimate with $r_{t+1}$ and discounted value from 2 (set the target value with TD estimate)  \n",
    "    4. calculate the TD error from 3 and critic local's estimate of $s_t, a_t$ to update critic_local (update Critic with TD error)  \n",
    "    5. get actor_local's predicted actions from the states, then estimate their values from the critic_local to update actor_local (the predicted actions can be different from $a_t$ as actor_local)  \n",
    "    6. update target networks via soft updates  \n",
    "    \n",
    "<img src=\"assets/actor_critic_update.png\" width=\"800px\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ornstein-Uhlenbeck process implementation from ShangtongZhang\n",
    "class RandomProcess(object):\n",
    "    def reset_states(self):\n",
    "        pass\n",
    "\n",
    "class OrnsteinUhlenbeckProcess(RandomProcess):\n",
    "    def __init__(self, size, seed, mu=0., std=0.2, theta=.15, dt=1e-2, x0=None):\n",
    "        self.size = size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        self.mu = mu\n",
    "        self.std = std\n",
    "        self.theta = theta\n",
    "        self.dt = dt\n",
    "        \n",
    "        self.x0 = x0\n",
    "        self.reset_states()\n",
    "        \n",
    "    def sample(self):\n",
    "        x = self.x_prev + \\\n",
    "            self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "            self.std * np.sqrt(self.dt) * np.random.randn(*self.size)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, seed, device, action_size, buffer_size, batch_size):\n",
    "        self.DEVICE = device\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\",\n",
    "                                                               \"action\",\n",
    "                                                               \"reward\",\n",
    "                                                               \"next_state\",\n",
    "                                                               \"done\"])\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(self.DEVICE)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(self.DEVICE)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.DEVICE)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(self.DEVICE)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(self.DEVICE)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "import os\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "class DDPGAgent():\n",
    "    def __init__(self, random_seed, device, n_agents, state_size, action_size,\n",
    "                buffer_size, batch_size, gamma, tau, lr_actor, lr_critic, weight_decay, checkpoint_folder= \"ckpt\"):\n",
    "        \n",
    "        if not os.path.exists(checkpoint_folder):\n",
    "            os.makedirs(checkpoint_folder)\n",
    "        \n",
    "        self.seed = random.seed(random_seed)\n",
    "        self.DEVICE = device\n",
    "        self.n_agents = n_agents\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # hyper-paramters\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.lr_actor = lr_actor\n",
    "        self.lr_critic = lr_critic\n",
    "        self.weight_decay = weight_decay\n",
    "        self.checkpoint_folder = checkpoint_folder\n",
    "        \n",
    "        # local and target actors and critics\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(self.DEVICE)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(self.DEVICE)\n",
    "        self.actor_local_optimizer = optim.Adam(self.actor_local.parameters(), lr=self.lr_actor)\n",
    "        \n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(self.DEVICE)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(self.DEVICE)\n",
    "        self.critic_local_optimizer = optim.Adam(self.critic_local.parameters(), lr=self.lr_critic)\n",
    "        \n",
    "        # Ornstein-Uhlenbeck noise process\n",
    "        self.noise = OrnsteinUhlenbeckProcess((n_agents, action_size), random_seed)\n",
    "        \n",
    "        # Replay Buffer\n",
    "        self.memory = ReplayBuffer(random_seed, device, action_size, self.buffer_size, self.batch_size)\n",
    "        \n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        1. append experience to the replay memory\n",
    "        2. take random samples from the replay memory\n",
    "        3. learn from those samples\n",
    "        \"\"\"\n",
    "        \n",
    "        #1\n",
    "        for i in range(self.n_agents):\n",
    "            self.memory.add(state[i], action[i], reward[i], next_state[i], done[i])\n",
    "            \n",
    "        \n",
    "        #2\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            samples_drawn = self.memory.sample()\n",
    "            \n",
    "            #3\n",
    "            self.learn(samples_drawn)\n",
    "            \n",
    "    def act(self, state, add_noise=True):\n",
    "        state = torch.from_numpy(state).float().to(self.DEVICE)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            ohnoise = self.noise.sample()\n",
    "            action += ohnoise\n",
    "        return np.clip(action, -1, 1)\n",
    "            \n",
    "    \n",
    "    def reset(self):\n",
    "        self.noise.reset_states()\n",
    "        \n",
    "    def learn(self, experiences):\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        # update critic\n",
    "        #1. use actor_target to predict the next actions for the next states\n",
    "        predicted_next_actions = self.actor_target(next_states)\n",
    "        \n",
    "        #2. use critic_target to estimate the value of the predicted next actions\n",
    "        estimated_values_of_predicted_next_actions = self.critic_target(next_states, predicted_next_actions)\n",
    "        \n",
    "        #3. calculate Q targets with the immediate reward and discounted value of #2\n",
    "        #if the next state is done, discounted #2 is 0\n",
    "        Q_targets = rewards + (self.gamma * estimated_values_of_predicted_next_actions * (1-dones))\n",
    "        \n",
    "        #4. use critic_local to estimate Q expected from states and actions of the sample experience\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        \n",
    "        #5. calculate MSE loss between Q expected and Q targets\n",
    "        #Q_targets based on critic_target, Q_expected based on critic_local\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        #6. minimise #5 using optimizer and backward to update critic_local\n",
    "        #that is learning the value of current state and action\n",
    "        self.critic_local_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_local_optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # update actor\n",
    "        #1 use actor_local to predict the next action without Ornstein-Uhlenbeck noise\n",
    "        #this shows actor_local's current progress\n",
    "        predicted_actions_wo_noise = self.actor_local(states)\n",
    "        \n",
    "        #2 use critic_local to estimate the value of actor_local's predicted actions\n",
    "        #take mean of the values of (states, actions)\n",
    "        estimated_value_of_predicted_actions = self.critic_local(states, predicted_actions_wo_noise).mean()\n",
    "        \n",
    "        #3 loss is the minus of #2 as we use gradient descient to maximize the estimated value\n",
    "        actor_loss = -estimated_value_of_predicted_actions\n",
    "        \n",
    "        #4 minimize #3 to update actor_local\n",
    "        self.actor_local_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_local_optimizer.step()\n",
    "        \n",
    "        # update target network using soft update\n",
    "        self.soft_update(self.critic_local, self.critic_target)\n",
    "        self.soft_update(self.actor_local, self.actor_target)\n",
    "        \n",
    "        \n",
    "    def soft_update(self, local_nn, target_nn):\n",
    "        for target_parameter, local_parameter in zip(target_nn.parameters(), local_nn.parameters()):\n",
    "            target_parameter.data.copy_(self.tau * local_parameter.data + (1.0 - self.tau) * target_parameter.data)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def save_checkpoints(self, suffix):\n",
    "        if not os.path.exists(self.checkpoint_folder):\n",
    "            os.makedirs(self.checkpoint_folder)\n",
    "        torch.save(self.actor_local.state_dict(), self.checkpoint_folder + '/actor_{}.pth'.format(str(suffix)))\n",
    "        torch.save(self.critic_local.state_dict(), self.checkpoint_folder + '/critic_{}.pth'.format(str(suffix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg_train(agent, n_agents, n_episodes, len_scores=100):\n",
    "    \n",
    "    #base_score\n",
    "    base_score = 0\n",
    "    \n",
    "    #score logging\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=len_scores)\n",
    "    \n",
    "    #for every episode\n",
    "    for episode in range(n_episodes):\n",
    "        \n",
    "        #reset env\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        \n",
    "        #get states\n",
    "        states = env_info.vector_observations\n",
    "        \n",
    "        #reset agent\n",
    "        agent.reset()\n",
    "        \n",
    "        #reset score\n",
    "        score = np.zeros(n_agents)\n",
    "        \n",
    "        #run until game ends\n",
    "        while True:\n",
    "            \n",
    "            # decide actions for the current state\n",
    "            actions = agent.act(states)\n",
    "            \n",
    "            # execute actions\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            \n",
    "            # get next_states, rewards, dones\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            \n",
    "            # learn the agent\n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            \n",
    "            # update score\n",
    "            score += rewards\n",
    "            \n",
    "            # update current states with the next_states\n",
    "            states = next_states\n",
    "            \n",
    "            # if any of the agents are done, break\n",
    "            if np.any(dones):\n",
    "                break\n",
    "                \n",
    "        scores.append(np.mean(score))\n",
    "        scores_window.append(np.mean(score))\n",
    "        \n",
    "        # save actor and critic checkpoints\n",
    "        if np.mean(scores_window) > base_score:\n",
    "            agent.save_checkpoints(\"score{}\".format(int(np.mean(scores_window))))\n",
    "            base_score += 10\n",
    "        \n",
    "        print('\\rEpisode: \\t{} \\tScore: \\t{:.2f} \\tAverage Score: \\t{:.2f}'.format(episode, np.mean(score), np.mean(scores_window)), end=\"\")  \n",
    "        \n",
    "        if np.mean(scores_window) >= 30.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_window)))\n",
    "            agent.save_checkpoints(\"solved\")\n",
    "            break\n",
    "            \n",
    "    return agent, scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: \t119 \tScore: \t32.44 \tAverage Score: \t30.26\n",
      "Environment solved in 119 episodes!\tAverage Score: 30.26\n"
     ]
    }
   ],
   "source": [
    "agent = DDPGAgent(random_seed=42, \n",
    "                  device=device, \n",
    "                  n_agents=num_agents, \n",
    "                  state_size=state_size,\n",
    "                  action_size=action_size, \n",
    "                  buffer_size=int(1e5), \n",
    "                  batch_size=128, \n",
    "                  gamma=0.99, \n",
    "                  tau=0.001, \n",
    "                  lr_actor=1e-4, \n",
    "                  lr_critic=1e-4, \n",
    "                  weight_decay=0.0)\n",
    "\n",
    "learned_agent, scores = ddpg_train(agent, n_agents=num_agents, n_episodes=1000, len_scores=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(1, len(scores)+1), scores)\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_xlabel(\"Episode #\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Test Result\n",
    "\n",
    "## Agent - score 0\n",
    "<img src=\"assets/reacher_score0.gif\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "## Agent - score 10\n",
    "<img src=\"assets/reacher_score10.gif\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "## Agent - score 30 (solved)\n",
    "<img src=\"assets/reacher_solved.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Ideas for Future Work\n",
    "\n",
    "In this project I used DDPG algorithm to train an agent and it successfully solved Reacher task. Here are some ideas to improve agent performance and make training more efficient:\n",
    "\n",
    "- Batch Normalization: I did not use batch-normalization for this version of implementation. As it's proven to help stabilize training with deep networks I should give it a go when solving more complex tasks.\n",
    "- Different noise process: DDPG uses Ornstein-Uhlenbeck process to add exploration noise during training. I wonder if other noise process would work or perform even better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
